{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e788179-b080-47b6-8215-ecf31bef21ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM v2: Bulletproof Processing with Checkpoints (1000 logs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"LLM v2: Bulletproof Processing with Checkpoints (1000 logs)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a0bef25-7f79-427b-8d86-ce5f7e9fbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclassified logs available: 14972\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint system\n",
    "checkpoint_dir = Path(\"llm_checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load environment and data\n",
    "load_dotenv()\n",
    "df = pd.read_csv('../data/nova_logs_with_bert.csv')\n",
    "unclassified_logs = df[(df['regex_label'].isnull()) & (df['bert_label'].isnull())].copy()\n",
    "print(f\"Unclassified logs available: {len(unclassified_logs)}\")\n",
    "\n",
    "# Global variables for tracking\n",
    "RESULTS_LIST = []\n",
    "PROCESSED_COUNT = 0\n",
    "CONSECUTIVE_FAILURES = 0\n",
    "MAX_CONSECUTIVE_FAILURES = 5  # Stop after 5 consecutive failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d018ea0c-d31e-452d-9267-f92eaebf24a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from: 0 logs already processed\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint_immediately(results_list, count, filename=\"llm_v2_checkpoint.pkl\"):\n",
    "    \"\"\"Save checkpoint immediately - never lose results\"\"\"\n",
    "    checkpoint_data = {\n",
    "        'results': results_list,\n",
    "        'processed_count': count,\n",
    "        'timestamp': datetime.datetime.now(),\n",
    "        'success_rate': len([r for r in results_list if r.category not in ['Processing_Error', 'Rate_Limit_Error']]) / max(len(results_list), 1)\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = checkpoint_dir / filename\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "    \n",
    "    # Also save as CSV for safety\n",
    "    if results_list:\n",
    "        results_data = []\n",
    "        for i, result in enumerate(results_list):\n",
    "            results_data.append({\n",
    "                'log_index': i,\n",
    "                'llm_category': result.category,\n",
    "                'llm_confidence': result.confidence,\n",
    "                'llm_reasoning': result.reasoning\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        results_df.to_csv(f'llm_v2_results_backup_{count}.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint saved: {count} logs processed\")\n",
    "\n",
    "def load_existing_checkpoint(filename=\"llm_v2_checkpoint.pkl\"):\n",
    "    \"\"\"Load existing checkpoint if available\"\"\"\n",
    "    checkpoint_path = checkpoint_dir / filename\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"üìÅ Checkpoint found: {checkpoint_data['processed_count']} logs previously processed\")\n",
    "        print(f\"üìä Previous success rate: {checkpoint_data['success_rate']:.2%}\")\n",
    "        return checkpoint_data['results'], checkpoint_data['processed_count']\n",
    "    \n",
    "    return [], 0\n",
    "\n",
    "# Load any existing checkpoint\n",
    "RESULTS_LIST, PROCESSED_COUNT = load_existing_checkpoint()\n",
    "print(f\"Starting from: {PROCESSED_COUNT} logs already processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cd7002e-6785-4078-bf0b-2bebf5f9d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategic sample created: 1000 logs to process\n"
     ]
    }
   ],
   "source": [
    "def create_strategic_1k_sample(unclassified_logs, target_size=1000, skip_processed=0):\n",
    "    \"\"\"Create strategic sample of 1k logs, skipping already processed\"\"\"\n",
    "    \n",
    "    strategic_sample = []\n",
    "    \n",
    "    # Priority 1: ERROR and WARNING logs (400 logs - 40%)\n",
    "    error_logs = unclassified_logs[\n",
    "        unclassified_logs['raw_log_text'].str.contains(\n",
    "            'ERROR|WARNING|CRITICAL|TIMEOUT|FAILED', case=False, na=False\n",
    "        )\n",
    "    ]\n",
    "    priority_1 = error_logs.sample(n=min(400, len(error_logs)), random_state=42)\n",
    "    strategic_sample.append(priority_1)\n",
    "    \n",
    "    # Priority 2: Cluster-based sampling (600 logs - 60%)\n",
    "    cluster_targets = {3: 200, 5: 150, 6: 150, 9: 50, 13: 50}\n",
    "    \n",
    "    used_indices = priority_1.index\n",
    "    for cluster_id, target_count in cluster_targets.items():\n",
    "        cluster_logs = unclassified_logs[\n",
    "            (unclassified_logs['cluster_id'] == cluster_id) & \n",
    "            (~unclassified_logs.index.isin(used_indices))\n",
    "        ]\n",
    "        \n",
    "        if len(cluster_logs) > 0:\n",
    "            sample_size = min(target_count, len(cluster_logs))\n",
    "            cluster_sample = cluster_logs.sample(n=sample_size, random_state=42)\n",
    "            strategic_sample.append(cluster_sample)\n",
    "            used_indices = used_indices.union(cluster_sample.index)\n",
    "    \n",
    "    # Combine and skip already processed\n",
    "    final_sample = pd.concat(strategic_sample, ignore_index=True)\n",
    "    \n",
    "    if skip_processed > 0:\n",
    "        final_sample = final_sample.iloc[skip_processed:].reset_index(drop=True)\n",
    "        print(f\"üìã Skipping {skip_processed} already processed logs\")\n",
    "    \n",
    "    return final_sample\n",
    "\n",
    "# Create sample, skipping already processed logs\n",
    "strategic_1k_logs = create_strategic_1k_sample(unclassified_logs, target_size=1000, skip_processed=PROCESSED_COUNT)\n",
    "print(f\"Strategic sample created: {len(strategic_1k_logs)} logs to process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dd29cc0-978a-4642-bd0e-fc761e064308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Loading .env from: /Users/kxshrx/dev/log-classification/.env\n",
      "LLM connection successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Path to .env file in the parent directory\n",
    "dotenv_path = os.path.abspath(os.path.join(os.getcwd(), '../.env'))\n",
    "print(f\"üìç Loading .env from: {dotenv_path}\")\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LangChain Groq client with llama-3.1-8b-instant\n",
    "try:\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "        model_name='llama-3.1-8b-instant',  # 5x more tokens than 70b\n",
    "        temperature=0.3,\n",
    "        max_tokens=120  # Reduced for efficiency\n",
    "    )\n",
    "    \n",
    "    # Test connection\n",
    "    test_response = llm.invoke([HumanMessage(content=\"Test connection. Reply 'OK'.\")])\n",
    "    print(\"LLM connection successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LLM initialization failed: {e}\")\n",
    "    print(\"Please check your GROQ_API_KEY in .env file\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a06287fd-0248-4fec-86bd-bd66c376b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced classification function with failure detection created\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model\n",
    "class LogClassification(BaseModel):\n",
    "    category: str = Field(..., description=\"Classification category\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score\")\n",
    "    reasoning: str = Field(..., description=\"Brief explanation\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "    model_name='llama-3.1-8b-instant',\n",
    "    temperature=0.3,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "# Optimized prompt\n",
    "optimized_template = \"\"\"Classify OpenStack log:\n",
    "\n",
    "CATEGORIES:\n",
    "SysOps, InstMgmt, NetOps, ResMgmt, SchedOps, BootErr, NetErr, FileErr, ConfigErr, ResErr, SvcErr\n",
    "\n",
    "EXAMPLES:\n",
    "- \"WARNING _wait_for_boot timeout\" ‚Üí BootErr\n",
    "- \"INFO VIF plugged successfully\" ‚Üí NetOps  \n",
    "- \"ERROR file not found\" ‚Üí FileErr\n",
    "\n",
    "LOG: {log_message}\n",
    "\n",
    "JSON: {{\"category\": \"X\", \"confidence\": 0.8, \"reasoning\": \"brief\"}}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"log_message\"], template=optimized_template)\n",
    "\n",
    "def classify_with_failure_detection(log_message, llm, prompt_template):\n",
    "    \"\"\"Classify with enhanced failure detection\"\"\"\n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format(log_message=log_message[:400])\n",
    "        messages = [HumanMessage(content=formatted_prompt)]\n",
    "        \n",
    "        response = llm.invoke(messages)\n",
    "        response_text = response.content.strip()\n",
    "        \n",
    "        # Parse JSON\n",
    "        import re\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            result_data = json.loads(json_str)\n",
    "        else:\n",
    "            result_data = json.loads(response_text)\n",
    "        \n",
    "        # Map categories\n",
    "        category_mapping = {\n",
    "            'SysOps': 'System_Operations', 'InstMgmt': 'Instance_Management', \n",
    "            'NetOps': 'Network_Operations', 'ResMgmt': 'Resource_Management',\n",
    "            'SchedOps': 'Scheduler_Operations', 'BootErr': 'Boot_Timeout_Errors',\n",
    "            'NetErr': 'Network_Connection_Errors', 'FileErr': 'File_System_Errors',\n",
    "            'ConfigErr': 'Configuration_Errors', 'ResErr': 'Resource_Allocation_Errors',\n",
    "            'SvcErr': 'Service_Communication_Errors'\n",
    "        }\n",
    "        \n",
    "        category = result_data.get('category', result_data.get('cat', 'Unknown'))\n",
    "        if category in category_mapping:\n",
    "            category = category_mapping[category]\n",
    "        \n",
    "        result = LogClassification(\n",
    "            category=category,\n",
    "            confidence=result_data.get('confidence', result_data.get('conf', 0.7)),\n",
    "            reasoning=result_data.get('reasoning', 'Classified')\n",
    "        )\n",
    "        \n",
    "        return result, False  # Success, no failure\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_str = str(e).lower()\n",
    "        \n",
    "        # Detect rate limit errors\n",
    "        if any(term in error_str for term in ['rate limit', '429', 'too many requests', 'quota']):\n",
    "            return LogClassification(\n",
    "                category=\"Rate_Limit_Error\",\n",
    "                confidence=0.0,\n",
    "                reasoning=\"Rate limit exceeded\"\n",
    "            ), True  # Failure detected\n",
    "        \n",
    "        # Other errors\n",
    "        return LogClassification(\n",
    "            category=\"Processing_Error\",\n",
    "            confidence=0.0,\n",
    "            reasoning=f\"Error: {str(e)[:50]}\"\n",
    "        ), True  # Failure detected\n",
    "\n",
    "print(\"Enhanced classification function with failure detection created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1c383a2-f3de-40e6-9084-e0ac07052e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Start time: 15:53:37\n",
      "üöÄ Starting processing from log 1/1000\n",
      "üìä Early stopping after 5 consecutive failures\n",
      " Failure 1/5: Processing_Error\n",
      " Failure 1/5: Rate_Limit_Error\n",
      " Failure 2/5: Processing_Error\n",
      " Failure 3/5: Processing_Error\n",
      " Failure 4/5: Processing_Error\n",
      " Failure 5/5: Rate_Limit_Error\n",
      "\n",
      "EARLY STOPPING: 5 consecutive failures detected\n",
      " Processed 7 logs before stopping\n",
      "‚úÖ Checkpoint saved: 7 logs processed\n",
      "\n",
      "‚úÖ Processing completed: 7 total classifications\n",
      "‚è∞ End time: 15:57:06\n",
      "‚è±Ô∏è Duration: 0:03:29.546919\n"
     ]
    }
   ],
   "source": [
    "def process_with_early_stopping_and_checkpoints(logs_list, llm, prompt_template, start_from=0):\n",
    "    \"\"\"Process logs with bulletproof checkpointing and early stopping\"\"\"\n",
    "    \n",
    "    global RESULTS_LIST, PROCESSED_COUNT, CONSECUTIVE_FAILURES\n",
    "    \n",
    "    total_logs = len(logs_list)\n",
    "    print(f\"üöÄ Starting processing from log {start_from + 1}/{total_logs}\")\n",
    "    print(f\"üìä Early stopping after {MAX_CONSECUTIVE_FAILURES} consecutive failures\")\n",
    "    \n",
    "    for idx in range(start_from, total_logs):\n",
    "        log_message = logs_list[idx]\n",
    "        \n",
    "        # Progress tracking\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            success_rate = len([r for r in RESULTS_LIST if r.category not in ['Processing_Error', 'Rate_Limit_Error']]) / max(len(RESULTS_LIST), 1)\n",
    "            print(f\"üìà Processed {idx + 1}/{total_logs} ({(idx+1)/total_logs*100:.1f}%) - Success rate: {success_rate:.1%}\")\n",
    "        \n",
    "        # Classify log\n",
    "        result, is_failure = classify_with_failure_detection(log_message, llm, prompt_template)\n",
    "        \n",
    "        # Add result to global list\n",
    "        RESULTS_LIST.append(result)\n",
    "        PROCESSED_COUNT += 1\n",
    "        \n",
    "        # Handle failures\n",
    "        if is_failure:\n",
    "            CONSECUTIVE_FAILURES += 1\n",
    "            print(f\" Failure {CONSECUTIVE_FAILURES}/{MAX_CONSECUTIVE_FAILURES}: {result.category}\")\n",
    "            \n",
    "            # Early stopping condition\n",
    "            if CONSECUTIVE_FAILURES >= MAX_CONSECUTIVE_FAILURES:\n",
    "                print(f\"\\nEARLY STOPPING: {CONSECUTIVE_FAILURES} consecutive failures detected\")\n",
    "                print(f\" Processed {len(RESULTS_LIST)} logs before stopping\")\n",
    "                break\n",
    "        else:\n",
    "            CONSECUTIVE_FAILURES = 0  # Reset on success\n",
    "        \n",
    "        # Save checkpoint every 25 logs\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            save_checkpoint_immediately(RESULTS_LIST, PROCESSED_COUNT)\n",
    "        \n",
    "        # Rate limiting delay\n",
    "        time.sleep(2.5)\n",
    "    \n",
    "    # Final checkpoint save\n",
    "    save_checkpoint_immediately(RESULTS_LIST, PROCESSED_COUNT)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processing completed: {len(RESULTS_LIST)} total classifications\")\n",
    "    return RESULTS_LIST\n",
    "\n",
    "# Execute processing\n",
    "strategic_logs_list = strategic_1k_logs['raw_log_text'].tolist()\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"‚è∞ Start time: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "final_results = process_with_early_stopping_and_checkpoints(\n",
    "    strategic_logs_list, \n",
    "    llm, \n",
    "    prompt,\n",
    "    start_from=0\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f\"‚è∞ End time: {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Duration: {end_time - start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2da2293-a112-4a1d-89ec-8fc6ddf53fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FINAL RESULTS ANALYSIS\n",
      "==================================================\n",
      "üìà Total processed: 7 logs\n",
      "\n",
      "üè∑Ô∏è Category Distribution:\n",
      "  Processing_Error: 4 (57.1%)\n",
      "  Rate_Limit_Error: 2 (28.6%)\n",
      "  Service_Communication_Errors: 1 (14.3%)\n",
      "\n",
      "üéØ Confidence Analysis:\n",
      "  Average confidence: 0.114\n",
      "  High confidence (‚â•0.65): 1/7 (14.3%)\n",
      "\n",
      "‚úÖ Success Rate: 14.3% (1/7)\n",
      "‚ùå Errors encountered: 7 (100.0%)\n",
      "\n",
      "üíæ FILES SAVED:\n",
      "‚úÖ llm_v2_final_results_1000.csv (7 classifications)\n",
      "‚úÖ strategic_1k_with_llm_results.csv (integration ready)\n",
      "‚úÖ Checkpoint files in llm_checkpoints/ directory\n",
      "\n",
      "üéâ LLM v2 PROCESSING COMPLETE\n",
      "üìä Successfully processed 7 logs with bulletproof error handling\n"
     ]
    }
   ],
   "source": [
    "# Safe analysis with no division by zero\n",
    "def safe_analyze_results(results_list):\n",
    "    \"\"\"Analyze results safely without division by zero errors\"\"\"\n",
    "    \n",
    "    if not results_list:\n",
    "        print(\"‚ö†Ô∏è No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä FINAL RESULTS ANALYSIS\")\n",
    "    print(f\"=\" * 50)\n",
    "    \n",
    "    # Extract data safely\n",
    "    categories = [r.category for r in results_list]\n",
    "    confidences = [r.confidence for r in results_list if isinstance(r.confidence, (int, float))]\n",
    "    \n",
    "    # Category distribution\n",
    "    category_dist = Counter(categories)\n",
    "    print(f\"üìà Total processed: {len(results_list)} logs\")\n",
    "    print(f\"\\nüè∑Ô∏è Category Distribution:\")\n",
    "    for category, count in category_dist.most_common():\n",
    "        percentage = count / len(results_list) * 100\n",
    "        print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Confidence analysis (safe)\n",
    "    if confidences:\n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        high_conf_count = len([c for c in confidences if c >= 0.65])\n",
    "        print(f\"\\nüéØ Confidence Analysis:\")\n",
    "        print(f\"  Average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"  High confidence (‚â•0.65): {high_conf_count}/{len(confidences)} ({high_conf_count/len(confidences)*100:.1f}%)\")\n",
    "    \n",
    "    # Success rate analysis\n",
    "    successful_classifications = len([r for r in results_list if r.category not in ['Processing_Error', 'Rate_Limit_Error']])\n",
    "    success_rate = successful_classifications / len(results_list) * 100\n",
    "    print(f\"\\n‚úÖ Success Rate: {success_rate:.1f}% ({successful_classifications}/{len(results_list)})\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = len([r for r in results_list if 'Error' in r.category])\n",
    "    if errors > 0:\n",
    "        print(f\"‚ùå Errors encountered: {errors} ({errors/len(results_list)*100:.1f}%)\")\n",
    "\n",
    "# Analyze results safely\n",
    "safe_analyze_results(RESULTS_LIST)\n",
    "\n",
    "# Save final results\n",
    "if RESULTS_LIST:\n",
    "    final_results_data = []\n",
    "    for i, result in enumerate(RESULTS_LIST):\n",
    "        final_results_data.append({\n",
    "            'log_index': i,\n",
    "            'llm_category': result.category,\n",
    "            'llm_confidence': result.confidence,\n",
    "            'llm_reasoning': result.reasoning\n",
    "        })\n",
    "    \n",
    "    final_df = pd.DataFrame(final_results_data)\n",
    "    final_df.to_csv('llm_v2_final_results_1000.csv', index=False)\n",
    "    \n",
    "    # Save integration-ready file\n",
    "    processed_logs = strategic_1k_logs.head(len(RESULTS_LIST)).copy()\n",
    "    processed_logs['llm_category'] = [r.category for r in RESULTS_LIST]\n",
    "    processed_logs['llm_confidence'] = [r.confidence for r in RESULTS_LIST]\n",
    "    processed_logs['llm_reasoning'] = [r.reasoning for r in RESULTS_LIST]\n",
    "    processed_logs.to_csv('strategic_1k_with_llm_results.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ FILES SAVED:\")\n",
    "    print(f\"‚úÖ llm_v2_final_results_1000.csv ({len(final_df)} classifications)\")\n",
    "    print(f\"‚úÖ strategic_1k_with_llm_results.csv (integration ready)\")\n",
    "    print(f\"‚úÖ Checkpoint files in llm_checkpoints/ directory\")\n",
    "\n",
    "print(f\"\\nüéâ LLM v2 PROCESSING COMPLETE\")\n",
    "print(f\"üìä Successfully processed {len(RESULTS_LIST)} logs with bulletproof error handling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d5d33-467e-4968-9883-66b66f23cfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
