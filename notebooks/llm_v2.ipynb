{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff64ade-1c83-45f0-a8a9-acdcab011356",
   "metadata": {},
   "source": [
    "# under development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33fdf82-802f-43da-9129-cd8b657c6706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM v2: Smart Sampling Approach\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"LLM v2: Smart Sampling Approach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd435e43-7163-49da-a4d6-ea7e543d05c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61950, 10)\n",
      "Total unclassified logs: 14972\n",
      "Available for LLM processing: 14972\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load BERT-processed dataset\n",
    "df = pd.read_csv('../data/nova_logs_with_bert.csv')\n",
    "print(df.shape)\n",
    "\n",
    "# Identify unclassified logs\n",
    "unclassified_logs = df[\n",
    "    (df['regex_label'].isnull()) & \n",
    "    (df['bert_label'].isnull())\n",
    "].copy()\n",
    "\n",
    "print(f\"Total unclassified logs: {len(unclassified_logs)}\")\n",
    "print(f\"Available for LLM processing: {len(unclassified_logs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0762457b-4733-48b5-a89b-d306018db336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority 1 (ERROR/WARNING): 600 logs\n",
      "Priority 2 (Small clusters): 400 logs\n",
      "Priority 3 (Random sample): 500 logs\n",
      "Smart sample created: 1500 logs\n"
     ]
    }
   ],
   "source": [
    "def create_smart_sample(unclassified_logs, target_size=1500):\n",
    "    \"\"\"Create smart sample prioritizing high-value logs\"\"\"\n",
    "    \n",
    "    smart_sample = []\n",
    "    \n",
    "    # Priority 1: ERROR and WARNING logs (highest value)\n",
    "    error_logs = unclassified_logs[\n",
    "        unclassified_logs['raw_log_text'].str.contains('ERROR|WARNING', case=False, na=False)\n",
    "    ]\n",
    "    priority_1 = error_logs.sample(n=min(600, len(error_logs)), random_state=42)\n",
    "    smart_sample.append(priority_1)\n",
    "    print(f\"Priority 1 (ERROR/WARNING): {len(priority_1)} logs\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Priority 2: Small cluster logs (edge cases)\n",
    "    small_cluster_logs = unclassified_logs[\n",
    "        unclassified_logs['cluster_id'].isin([13, 22, 16, 24, 23])\n",
    "    ]\n",
    "    remaining_small = small_cluster_logs[~small_cluster_logs.index.isin(priority_1.index)]\n",
    "    priority_2 = remaining_small.sample(n=min(400, len(remaining_small)), random_state=42)\n",
    "    smart_sample.append(priority_2)\n",
    "    print(f\"Priority 2 (Small clusters): {len(priority_2)} logs\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Priority 3: Random sample from remaining\n",
    "    used_indices = pd.concat(smart_sample).index\n",
    "    remaining_logs = unclassified_logs[~unclassified_logs.index.isin(used_indices)]\n",
    "    remaining_needed = target_size - sum(len(s) for s in smart_sample)\n",
    "    priority_3 = remaining_logs.sample(n=min(remaining_needed, len(remaining_logs)), random_state=42)\n",
    "    smart_sample.append(priority_3)\n",
    "    print(f\"Priority 3 (Random sample): {len(priority_3)} logs\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Combine all priorities\n",
    "    final_sample = pd.concat(smart_sample, ignore_index=True)\n",
    "    return final_sample\n",
    "\n",
    "# Create smart sample\n",
    "smart_sample_logs = create_smart_sample(unclassified_logs, target_size=1500)\n",
    "print(f\"Smart sample created: {len(smart_sample_logs)} logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ecceb9-7517-4e73-af4c-1de3c011e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced categories defined:\n",
      "  System_Operations: LibVirt driver operations, system-level tasks\n",
      "  Instance_Management: VM lifecycle, instance operations\n",
      "  Network_Operations: VIF operations, network connectivity\n",
      "  Resource_Management: Compute claims, resource allocation\n",
      "  Scheduler_Operations: Nova scheduler activities, allocation reports\n",
      "  Boot_Timeout_Errors: VM boot timeouts, startup failures\n",
      "  Network_Connection_Errors: VIF connection failures, network issues\n",
      "  Resource_Allocation_Errors: Memory/CPU allocation failures\n",
      "  File_System_Errors: File not found, permission errors, I/O failures\n",
      "  Configuration_Errors: Invalid config, missing parameters\n",
      "  Service_Communication_Errors: API timeouts, service unavailable\n"
     ]
    }
   ],
   "source": [
    "# Define Pydantic model for structured output\n",
    "class LogClassification(BaseModel):\n",
    "    category: str = Field(..., description=\"The classification category\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score\")\n",
    "    reasoning: str = Field(..., description=\"Brief explanation\")\n",
    "\n",
    "# Enhanced categories with error subcategories\n",
    "ENHANCED_CATEGORIES = {\n",
    "    'System_Operations': 'LibVirt driver operations, system-level tasks',\n",
    "    'Instance_Management': 'VM lifecycle, instance operations',\n",
    "    'Network_Operations': 'VIF operations, network connectivity', \n",
    "    'Resource_Management': 'Compute claims, resource allocation',\n",
    "    'Scheduler_Operations': 'Nova scheduler activities, allocation reports',\n",
    "    'Boot_Timeout_Errors': 'VM boot timeouts, startup failures',\n",
    "    'Network_Connection_Errors': 'VIF connection failures, network issues',\n",
    "    'Resource_Allocation_Errors': 'Memory/CPU allocation failures',\n",
    "    'File_System_Errors': 'File not found, permission errors, I/O failures',\n",
    "    'Configuration_Errors': 'Invalid config, missing parameters',\n",
    "    'Service_Communication_Errors': 'API timeouts, service unavailable'\n",
    "}\n",
    "\n",
    "print(\"Enhanced categories defined:\")\n",
    "for category, description in ENHANCED_CATEGORIES.items():\n",
    "    print(f\"  {category}: {description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9976de61-ad9c-4683-9648-5047ab09f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: /Users/kxshrx/dev/log-classification/.env\n",
      "Groq API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq  # Adjust import as needed\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load .env from parent directory\n",
    "dotenv_path = os.path.abspath(os.path.join(os.getcwd(), '../.env'))\n",
    "print(f\"Loading .env from: {dotenv_path}\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Verify GROQ_API_KEY\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "if api_key and api_key != 'your_groq_api_key_here':\n",
    "    print(\"Groq API key loaded successfully\")\n",
    "else:\n",
    "    raise ValueError(\"GROQ_API_KEY not set or invalid in .env file\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c82d14-5ec7-438f-b2c4-f28100d3ce22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Groq client initialized\n",
      "Optimized prompt template created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize LangChain Groq client\n",
    "try:\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "        model_name='llama-3.3-70b-versatile',\n",
    "        temperature=0.3,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    print(\"LangChain Groq client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing client: {e}\")\n",
    "\n",
    "# Create optimized prompt template (reduced tokens)\n",
    "classification_template = \"\"\"Classify this OpenStack log into the most specific category:\n",
    "\n",
    "CATEGORIES:\n",
    "1. System_Operations - LibVirt driver, system tasks\n",
    "2. Instance_Management - VM lifecycle operations  \n",
    "3. Network_Operations - VIF operations, connectivity\n",
    "4. Resource_Management - Compute claims, allocation\n",
    "5. Scheduler_Operations - Nova scheduler activities\n",
    "6. Boot_Timeout_Errors - VM boot timeouts, startup failures\n",
    "7. Network_Connection_Errors - VIF connection failures\n",
    "8. Resource_Allocation_Errors - Memory/CPU allocation failures\n",
    "9. File_System_Errors - File not found, I/O failures\n",
    "10. Configuration_Errors - Invalid config, setup issues\n",
    "11. Service_Communication_Errors - API timeouts, service unavailable\n",
    "\n",
    "RULES:\n",
    "- Focus on the primary operation or error type\n",
    "- Be specific with error subcategories\n",
    "- Provide confidence 0.6-1.0\n",
    "\n",
    "LOG: {log_message}\n",
    "\n",
    "JSON response:\n",
    "{{\"category\": \"category_name\", \"confidence\": 0.8, \"reasoning\": \"brief reason\"}}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"log_message\"],\n",
    "    template=classification_template\n",
    ")\n",
    "\n",
    "print(\"Optimized prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "777afbc2-4f97-46bd-9e92-c47228f59bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification function with uncertainty handling created\n"
     ]
    }
   ],
   "source": [
    "def classify_log_with_uncertainty(log_message, llm, prompt_template, confidence_threshold=0.7):\n",
    "    \"\"\"Classify log with uncertainty handling\"\"\"\n",
    "    try:\n",
    "        # Format prompt\n",
    "        formatted_prompt = prompt_template.format(log_message=log_message)\n",
    "        messages = [HumanMessage(content=formatted_prompt)]\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = llm.invoke(messages)\n",
    "        response_text = response.content.strip()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            # Extract JSON from response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                result_data = json.loads(json_str)\n",
    "            else:\n",
    "                result_data = json.loads(response_text)\n",
    "            \n",
    "            # Create Pydantic object\n",
    "            result = LogClassification(**result_data)\n",
    "            \n",
    "            # Apply uncertainty handling\n",
    "            if result.confidence >= confidence_threshold:\n",
    "                return result\n",
    "            else:\n",
    "                # Mark as uncertain with likely prediction\n",
    "                uncertain_result = LogClassification(\n",
    "                    category=f\"Uncertain (likely: {result.category})\",\n",
    "                    confidence=result.confidence,\n",
    "                    reasoning=f\"Low confidence: {result.reasoning}\"\n",
    "                )\n",
    "                return uncertain_result\n",
    "                \n",
    "        except (json.JSONDecodeError, Exception):\n",
    "            # Return default for parsing errors\n",
    "            return LogClassification(\n",
    "                category=\"Unknown\",\n",
    "                confidence=0.0,\n",
    "                reasoning=\"JSON parsing failed\"\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        return LogClassification(\n",
    "            category=\"Error\",\n",
    "            confidence=0.0,\n",
    "            reasoning=f\"LLM error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"Classification function with uncertainty handling created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95ad8b1e-d25d-409a-a929-183d5404d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing function with rate limit handling created\n"
     ]
    }
   ],
   "source": [
    "def process_batch_with_rate_limits(logs_batch, llm, prompt_template, delay=0.2):\n",
    "    \"\"\"Process batch with rate limit awareness\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(logs_batch)} logs...\")\n",
    "    \n",
    "    for idx, log_message in enumerate(logs_batch):\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(logs_batch)} logs\")\n",
    "        \n",
    "        try:\n",
    "            result = classify_log_with_uncertainty(log_message, llm, prompt_template)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Rate limiting delay\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                print(f\"Rate limit reached at log {idx + 1}\")\n",
    "                print(f\"Successfully processed {len(results)} logs before limit\")\n",
    "                break\n",
    "            else:\n",
    "                # Handle other errors\n",
    "                error_result = LogClassification(\n",
    "                    category=\"Error\",\n",
    "                    confidence=0.0,\n",
    "                    reasoning=f\"Processing error: {str(e)}\"\n",
    "                )\n",
    "                results.append(error_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Batch processing function with rate limit handling created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d420fdd2-80e1-4d7f-9a9a-910cb989963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting smart sample processing...\n",
      "This will process until rate limits are reached\n",
      "Processing 1500 logs...\n",
      "Processed 50/1500 logs\n",
      "Processed 100/1500 logs\n",
      "Processed 150/1500 logs\n",
      "Processed 200/1500 logs\n",
      "Processed 250/1500 logs\n",
      "Processed 300/1500 logs\n",
      "Processed 350/1500 logs\n",
      "Processed 400/1500 logs\n",
      "Processed 450/1500 logs\n",
      "Processed 500/1500 logs\n",
      "Processed 550/1500 logs\n",
      "Processed 600/1500 logs\n",
      "Processed 650/1500 logs\n",
      "Processed 700/1500 logs\n",
      "Processed 750/1500 logs\n",
      "Processed 800/1500 logs\n",
      "Processed 850/1500 logs\n",
      "Processed 900/1500 logs\n",
      "Processed 950/1500 logs\n",
      "Processed 1000/1500 logs\n",
      "Processed 1050/1500 logs\n",
      "Processed 1100/1500 logs\n",
      "Processed 1150/1500 logs\n",
      "Processed 1200/1500 logs\n",
      "Processed 1250/1500 logs\n",
      "Processed 1300/1500 logs\n",
      "Processed 1350/1500 logs\n",
      "Processed 1400/1500 logs\n",
      "Processed 1450/1500 logs\n",
      "Processed 1500/1500 logs\n",
      "Processing completed: 1500 logs classified\n",
      "Classification distribution:\n",
      "  Error: 1494\n",
      "  Instance_Management: 2\n",
      "  Unknown: 2\n",
      "  Resource_Allocation_Errors: 1\n",
      "  Network_Connection_Errors: 1\n"
     ]
    }
   ],
   "source": [
    "# Process smart sample with rate limit awareness\n",
    "print(\"Starting smart sample processing...\")\n",
    "print(\"This will process until rate limits are reached\")\n",
    "\n",
    "# Convert to list for processing\n",
    "smart_sample_texts = smart_sample_logs['raw_log_text'].tolist()\n",
    "\n",
    "# Process the smart sample\n",
    "classification_results = process_batch_with_rate_limits(\n",
    "    smart_sample_texts,\n",
    "    llm,\n",
    "    prompt,\n",
    "    delay=0.15  # Optimized delay\n",
    ")\n",
    "\n",
    "print(f\"Processing completed: {len(classification_results)} logs classified\")\n",
    "\n",
    "# Quick analysis of results\n",
    "categories = [r.category for r in classification_results]\n",
    "category_dist = Counter(categories)\n",
    "\n",
    "print(\"Classification distribution:\")\n",
    "for category, count in category_dist.most_common():\n",
    "    print(f\"  {category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35110d63-c10a-4d61-b08d-4958db6186e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM v2: Smart Sampling Approach - Clean Version\n",
      "Total unclassified logs: 14972\n",
      "Priority 1 (ERROR/WARNING): 600 logs\n",
      "Priority 2 (Small clusters): 400 logs\n",
      "Priority 3 (Random sample): 500 logs\n",
      "Smart sample created: 1500 logs\n",
      "Enhanced categories defined\n",
      "LangChain client initialized\n",
      "Prompt template created\n",
      "Processing 1500 logs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/groq/_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Process smart sample\u001b[39;00m\n\u001b[32m    196\u001b[39m smart_sample_texts = smart_sample_logs[\u001b[33m'\u001b[39m\u001b[33mraw_log_text\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m classification_results = \u001b[43mprocess_batch_with_rate_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmart_sample_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing completed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(classification_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m logs classified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mprocess_batch_with_rate_limits\u001b[39m\u001b[34m(logs_batch, llm, prompt_template, delay)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(logs_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m logs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     result = \u001b[43mclassify_log_with_uncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     results.append(result)\n\u001b[32m    177\u001b[39m     time.sleep(delay)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mclassify_log_with_uncertainty\u001b[39m\u001b[34m(log_message, llm, prompt_template, confidence_threshold)\u001b[39m\n\u001b[32m    125\u001b[39m formatted_prompt = prompt_template.format(log_message=log_message)\n\u001b[32m    126\u001b[39m messages = [HumanMessage(content=formatted_prompt)]\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m response_text = response.content.strip()\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Parse JSON response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/langchain_groq/chat_models.py:498\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    494\u001b[39m params = {\n\u001b[32m    495\u001b[39m     **params,\n\u001b[32m    496\u001b[39m     **kwargs,\n\u001b[32m    497\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/groq/resources/chat/completions.py:368\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    183\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    230\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    231\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    233\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/groq/_base_client.py:1225\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1212\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1213\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1220\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1221\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1222\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1223\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1224\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/groq/_base_client.py:1020\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1019\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/log-classification/logenv/lib/python3.12/site-packages/groq/_base_client.py:1060\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1057\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1058\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"LLM v2: Smart Sampling Approach - Clean Version\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load BERT-processed dataset\n",
    "df = pd.read_csv('../data/nova_logs_with_bert.csv')\n",
    "\n",
    "# Identify unclassified logs\n",
    "unclassified_logs = df[(df['regex_label'].isnull()) & (df['bert_label'].isnull())].copy()\n",
    "print(f\"Total unclassified logs: {len(unclassified_logs)}\")\n",
    "\n",
    "# Smart sampling function\n",
    "def create_smart_sample(unclassified_logs, target_size=1500):\n",
    "    smart_sample = []\n",
    "    \n",
    "    # Priority 1: ERROR and WARNING logs\n",
    "    error_logs = unclassified_logs[unclassified_logs['raw_log_text'].str.contains('ERROR|WARNING', case=False, na=False)]\n",
    "    priority_1 = error_logs.sample(n=min(600, len(error_logs)), random_state=42)\n",
    "    smart_sample.append(priority_1)\n",
    "    print(f\"Priority 1 (ERROR/WARNING): {len(priority_1)} logs\")\n",
    "    \n",
    "    # Priority 2: Small cluster logs\n",
    "    small_cluster_logs = unclassified_logs[unclassified_logs['cluster_id'].isin([13, 22, 16, 24, 23])]\n",
    "    remaining_small = small_cluster_logs[~small_cluster_logs.index.isin(priority_1.index)]\n",
    "    priority_2 = remaining_small.sample(n=min(400, len(remaining_small)), random_state=42)\n",
    "    smart_sample.append(priority_2)\n",
    "    print(f\"Priority 2 (Small clusters): {len(priority_2)} logs\")\n",
    "    \n",
    "    # Priority 3: Random sample from remaining\n",
    "    used_indices = pd.concat(smart_sample).index\n",
    "    remaining_logs = unclassified_logs[~unclassified_logs.index.isin(used_indices)]\n",
    "    remaining_needed = target_size - sum(len(s) for s in smart_sample)\n",
    "    priority_3 = remaining_logs.sample(n=min(remaining_needed, len(remaining_logs)), random_state=42)\n",
    "    smart_sample.append(priority_3)\n",
    "    print(f\"Priority 3 (Random sample): {len(priority_3)} logs\")\n",
    "    \n",
    "    final_sample = pd.concat(smart_sample, ignore_index=True)\n",
    "    return final_sample\n",
    "\n",
    "smart_sample_logs = create_smart_sample(unclassified_logs, target_size=1500)\n",
    "print(f\"Smart sample created: {len(smart_sample_logs)} logs\")\n",
    "\n",
    "# Pydantic model\n",
    "class LogClassification(BaseModel):\n",
    "    category: str = Field(..., description=\"The classification category\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score\")\n",
    "    reasoning: str = Field(..., description=\"Brief explanation\")\n",
    "\n",
    "# Enhanced categories\n",
    "ENHANCED_CATEGORIES = {\n",
    "    'System_Operations': 'LibVirt driver operations, system-level tasks',\n",
    "    'Instance_Management': 'VM lifecycle, instance operations',\n",
    "    'Network_Operations': 'VIF operations, network connectivity', \n",
    "    'Resource_Management': 'Compute claims, resource allocation',\n",
    "    'Scheduler_Operations': 'Nova scheduler activities, allocation reports',\n",
    "    'Boot_Timeout_Errors': 'VM boot timeouts, startup failures',\n",
    "    'Network_Connection_Errors': 'VIF connection failures, network issues',\n",
    "    'Resource_Allocation_Errors': 'Memory/CPU allocation failures',\n",
    "    'File_System_Errors': 'File not found, permission errors, I/O failures',\n",
    "    'Configuration_Errors': 'Invalid config, missing parameters',\n",
    "    'Service_Communication_Errors': 'API timeouts, service unavailable'\n",
    "}\n",
    "\n",
    "print(\"Enhanced categories defined\")\n",
    "\n",
    "# Initialize LangChain Groq client\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "    model_name='llama-3.3-70b-versatile',\n",
    "    temperature=0.3,\n",
    "    max_tokens=150\n",
    ")\n",
    "print(\"LangChain client initialized\")\n",
    "\n",
    "# Optimized prompt template (fixed escape sequences)\n",
    "classification_template = \"\"\"Classify this OpenStack log into the most specific category:\n",
    "\n",
    "CATEGORIES:\n",
    "1. System_Operations - LibVirt driver, system tasks\n",
    "2. Instance_Management - VM lifecycle operations  \n",
    "3. Network_Operations - VIF operations, connectivity\n",
    "4. Resource_Management - Compute claims, allocation\n",
    "5. Scheduler_Operations - Nova scheduler activities\n",
    "6. Boot_Timeout_Errors - VM boot timeouts, startup failures\n",
    "7. Network_Connection_Errors - VIF connection failures\n",
    "8. Resource_Allocation_Errors - Memory/CPU allocation failures\n",
    "9. File_System_Errors - File not found, I/O failures\n",
    "10. Configuration_Errors - Invalid config, setup issues\n",
    "11. Service_Communication_Errors - API timeouts, service unavailable\n",
    "\n",
    "RULES:\n",
    "- Focus on the primary operation or error type\n",
    "- Be specific with error subcategories\n",
    "- Provide confidence 0.6-1.0\n",
    "\n",
    "LOG: {log_message}\n",
    "\n",
    "JSON response:\n",
    "{{\"category\": \"category_name\", \"confidence\": 0.8, \"reasoning\": \"brief reason\"}}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"log_message\"],\n",
    "    template=classification_template\n",
    ")\n",
    "print(\"Prompt template created\")\n",
    "\n",
    "# Classification function with uncertainty handling\n",
    "def classify_log_with_uncertainty(log_message, llm, prompt_template, confidence_threshold=0.7):\n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format(log_message=log_message)\n",
    "        messages = [HumanMessage(content=formatted_prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        response_text = response.content.strip()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        import re\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            result_data = json.loads(json_str)\n",
    "        else:\n",
    "            result_data = json.loads(response_text)\n",
    "        \n",
    "        result = LogClassification(**result_data)\n",
    "        \n",
    "        # Apply uncertainty handling\n",
    "        if result.confidence >= confidence_threshold:\n",
    "            return result\n",
    "        else:\n",
    "            uncertain_result = LogClassification(\n",
    "                category=f\"Uncertain (likely: {result.category})\",\n",
    "                confidence=result.confidence,\n",
    "                reasoning=f\"Low confidence: {result.reasoning}\"\n",
    "            )\n",
    "            return uncertain_result\n",
    "            \n",
    "    except (json.JSONDecodeError, Exception):\n",
    "        return LogClassification(\n",
    "            category=\"Unknown\",\n",
    "            confidence=0.0,\n",
    "            reasoning=\"JSON parsing failed\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return LogClassification(\n",
    "            category=\"Error\",\n",
    "            confidence=0.0,\n",
    "            reasoning=f\"LLM error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# Batch processing with rate limit awareness\n",
    "def process_batch_with_rate_limits(logs_batch, llm, prompt_template, delay=0.15):\n",
    "    results = []\n",
    "    print(f\"Processing {len(logs_batch)} logs...\")\n",
    "    \n",
    "    for idx, log_message in enumerate(logs_batch):\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(logs_batch)} logs\")\n",
    "        \n",
    "        try:\n",
    "            result = classify_log_with_uncertainty(log_message, llm, prompt_template)\n",
    "            results.append(result)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            if \"rate limit\" in error_str or \"429\" in error_str or \"too many requests\" in error_str:\n",
    "                print(f\"Rate limit reached at log {idx + 1}\")\n",
    "                print(f\"Successfully processed {len(results)} logs before limit\")\n",
    "                break\n",
    "            else:\n",
    "                error_result = LogClassification(\n",
    "                    category=\"Processing_Error\",\n",
    "                    confidence=0.0,\n",
    "                    reasoning=f\"Processing error: {str(e)}\"\n",
    "                )\n",
    "                results.append(error_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process smart sample\n",
    "smart_sample_texts = smart_sample_logs['raw_log_text'].tolist()\n",
    "classification_results = process_batch_with_rate_limits(smart_sample_texts, llm, prompt, delay=0.15)\n",
    "\n",
    "print(f\"Processing completed: {len(classification_results)} logs classified\")\n",
    "\n",
    "# Analyze results\n",
    "categories = [r.category for r in classification_results]\n",
    "category_dist = Counter(categories)\n",
    "\n",
    "print(\"Classification distribution:\")\n",
    "for category, count in category_dist.most_common():\n",
    "    print(f\"  {category}: {count}\")\n",
    "\n",
    "# Calculate success rate\n",
    "successful_classifications = [r for r in classification_results if r.category not in ['Error', 'Processing_Error', 'Unknown']]\n",
    "success_rate = len(successful_classifications) / len(classification_results) * 100\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_data = []\n",
    "for i, result in enumerate(classification_results):\n",
    "    results_data.append({\n",
    "        'log_index': i,\n",
    "        'llm_category': result.category,\n",
    "        'llm_confidence': result.confidence,\n",
    "        'llm_reasoning': result.reasoning\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('../data/nova_logs_with_llm_v2.csv', index=False)\n",
    "print(\"Results saved to nova_logs_with_llm_v2.csv\")\n",
    "\n",
    "print(\"LLM v2 processing complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
