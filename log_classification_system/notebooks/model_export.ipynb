{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb124d5",
   "metadata": {},
   "source": [
    "# Export Fine-Tuned BERT Model for Hugging Face Hub\n",
    "\n",
    "This notebook loads the fine-tuned controlled_bert_model.pth and saves it in the proper Hugging Face format with all necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6056f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertConfig\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef518e7",
   "metadata": {},
   "source": [
    "## Step 1: Recreate the Label Mapping\n",
    "\n",
    "First, we need to recreate the exact label mapping that was used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ee5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 54646 logs\n",
      "\n",
      "Comprehensive dataset created with 45174 samples\n",
      "Label distribution:\n",
      "bert_training_label\n",
      "Instance_Management     25378\n",
      "System_Operations        9863\n",
      "Network_Operations       4067\n",
      "Resource_Management      2467\n",
      "Scheduler_Operations     2462\n",
      "Error_Handling            937\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset to recreate the label mapping\n",
    "df = pd.read_csv('../results/nova_logs_with_regex.csv')\n",
    "print(f\"Loaded dataset with {len(df)} logs\")\n",
    "\n",
    "def create_comprehensive_bert_dataset(df):\n",
    "    \"\"\"Recreate the exact dataset processing used during training\"\"\"\n",
    "    \n",
    "    # Regex-classified logs (convert regex labels to BERT labels)\n",
    "    regex_classified = df[df['regex_label'].notnull()].copy()\n",
    "    \n",
    "    # Unclassified logs from target clusters only\n",
    "    bert_target_clusters = [3, 5, 6, 9, 13]\n",
    "    unclassified = df[df['regex_label'].isnull() & df['cluster_id'].isin(bert_target_clusters)].copy()\n",
    "    \n",
    "    # Create unified label mapping\n",
    "    unified_labels = {\n",
    "        # From regex categories\n",
    "        'System_Operations_LibVirt': 'System_Operations',\n",
    "        'Instance_Management_Compute': 'Instance_Management', \n",
    "        'Instance_Management_System': 'Instance_Management',\n",
    "        \n",
    "        # From clusters (semantic labels)\n",
    "        3: 'Network_Operations',      # os_vif operations\n",
    "        5: 'Resource_Management',     # compute claims\n",
    "        6: 'Scheduler_Operations',    # scheduler reports\n",
    "        9: 'Network_Operations',      # VIF operations (merge with cluster 3)\n",
    "        13: 'Error_Handling'          # error patterns\n",
    "    }\n",
    "    \n",
    "    # Apply unified labels to regex-classified logs\n",
    "    regex_classified['bert_training_label'] = regex_classified['regex_label'].map(unified_labels)\n",
    "    \n",
    "    # Apply unified labels to unclassified logs\n",
    "    unclassified['bert_training_label'] = unclassified['cluster_id'].map(unified_labels)\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_data = pd.concat([\n",
    "        regex_classified[['raw_log_text', 'bert_training_label']], \n",
    "        unclassified[['raw_log_text', 'bert_training_label']]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Remove any nulls\n",
    "    combined_data = combined_data.dropna()\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Recreate the training dataset\n",
    "comprehensive_data = create_comprehensive_bert_dataset(df)\n",
    "print(f\"\\nComprehensive dataset created with {len(comprehensive_data)} samples\")\n",
    "print(f\"Label distribution:\")\n",
    "print(comprehensive_data['bert_training_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8218c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 6\n",
      "Label classes: ['Error_Handling' 'Instance_Management' 'Network_Operations'\n",
      " 'Resource_Management' 'Scheduler_Operations' 'System_Operations']\n",
      "Label mapping: {'Error_Handling': 0, 'Instance_Management': 1, 'Network_Operations': 2, 'Resource_Management': 3, 'Scheduler_Operations': 4, 'System_Operations': 5}\n",
      "\n",
      "id2label mapping: {0: 'Error_Handling', 1: 'Instance_Management', 2: 'Network_Operations', 3: 'Resource_Management', 4: 'Scheduler_Operations', 5: 'System_Operations'}\n",
      "label2id mapping: {'Error_Handling': 0, 'Instance_Management': 1, 'Network_Operations': 2, 'Resource_Management': 3, 'Scheduler_Operations': 4, 'System_Operations': 5}\n"
     ]
    }
   ],
   "source": [
    "# Recreate the label encoder with the exact same mapping\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(comprehensive_data['bert_training_label'])\n",
    "\n",
    "print(f\"Number of labels: {len(label_encoder.classes_)}\")\n",
    "print(f\"Label classes: {label_encoder.classes_}\")\n",
    "print(f\"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# Store the label mapping for later use\n",
    "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "print(f\"\\nid2label mapping: {id2label}\")\n",
    "print(f\"label2id mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74ba1",
   "metadata": {},
   "source": [
    "## Step 2: Load and Initialize the Base Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e3044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: distilbert-base-uncased\n",
      "Number of labels: 6\n",
      "Tokenizer loaded successfully\n",
      "Tokenizer loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded with 6 labels\n",
      "Model configuration: DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Error_Handling\",\n",
      "    \"1\": \"Instance_Management\",\n",
      "    \"2\": \"Network_Operations\",\n",
      "    \"3\": \"Resource_Management\",\n",
      "    \"4\": \"Scheduler_Operations\",\n",
      "    \"5\": \"System_Operations\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"Error_Handling\": 0,\n",
      "    \"Instance_Management\": 1,\n",
      "    \"Network_Operations\": 2,\n",
      "    \"Resource_Management\": 3,\n",
      "    \"Scheduler_Operations\": 4,\n",
      "    \"System_Operations\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model configuration - same as used during training\n",
    "model_name = 'distilbert-base-uncased'\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded successfully\")\n",
    "\n",
    "# Create the model with the same configuration used during training\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded with {num_labels} labels\")\n",
    "print(f\"Model configuration: {model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003bc48",
   "metadata": {},
   "source": [
    "## Step 3: Load the Fine-Tuned Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1828a41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned weights from: ../models/controlled_bert_model.pth\n",
      "Fine-tuned weights loaded successfully!\n",
      "Model is now ready for inference\n",
      "Fine-tuned weights loaded successfully!\n",
      "Model is now ready for inference\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned weights\n",
    "model_path = '../models/controlled_bert_model.pth'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading fine-tuned weights from: {model_path}\")\n",
    "    \n",
    "    # Load the state dict\n",
    "    state_dict = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Load the weights into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    print(\"Fine-tuned weights loaded successfully!\")\n",
    "    print(f\"Model is now ready for inference\")\n",
    "else:\n",
    "    print(f\"ERROR: Model file not found at {model_path}\")\n",
    "    print(\"Please make sure you have trained and saved the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4d524",
   "metadata": {},
   "source": [
    "## Step 4: Test the Model to Verify It Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edb50662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with sample log:\n",
      "'INFO nova.compute.manager [req-b9d6411c-b3ea-4307-a707-ec546b0192b3] [instance: 8192614e-4a86-47cc-a...'\n",
      "\n",
      "Prediction successful!\n",
      "Predicted label: Instance_Management\n",
      "Confidence: 0.7135\n",
      "Model is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample log to make sure it works\n",
    "model.eval()\n",
    "\n",
    "# Get a sample log from the dataset\n",
    "sample_log = comprehensive_data['raw_log_text'].iloc[0]\n",
    "print(f\"Testing with sample log:\")\n",
    "print(f\"'{sample_log[:100]}...'\")\n",
    "\n",
    "# Tokenize and predict\n",
    "inputs = tokenizer(sample_log, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class_id = predictions.argmax().item()\n",
    "    predicted_label = id2label[predicted_class_id]\n",
    "    confidence = predictions[0][predicted_class_id].item()\n",
    "\n",
    "print(f\"\\nPrediction successful!\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "print(f\"Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73145325",
   "metadata": {},
   "source": [
    "## Step 5: Save Model in Hugging Face Format\n",
    "\n",
    "This is the most important step - saving all the files needed for Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac0eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer to: ../models/infrnce_bert_model_complete\n",
      "âœ“ Model saved (pytorch_model.bin, config.json created)\n",
      "âœ“ Tokenizer saved (tokenizer.json, vocab.txt, etc. created)\n",
      "\n",
      "Model export completed successfully!\n",
      "All files are now in: /Users/kxshrx/dev/infrnce/log_classification_system/notebooks/../models/infrnce_bert_model_complete\n",
      "âœ“ Model saved (pytorch_model.bin, config.json created)\n",
      "âœ“ Tokenizer saved (tokenizer.json, vocab.txt, etc. created)\n",
      "\n",
      "Model export completed successfully!\n",
      "All files are now in: /Users/kxshrx/dev/infrnce/log_classification_system/notebooks/../models/infrnce_bert_model_complete\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory in the models folder\n",
    "output_dir = Path('../models/infrnce_bert_model_complete')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving model and tokenizer to: {output_dir}\")\n",
    "\n",
    "# Save the model (this creates pytorch_model.bin and config.json)\n",
    "model.save_pretrained(output_dir)\n",
    "print(\"âœ“ Model saved (pytorch_model.bin, config.json created)\")\n",
    "\n",
    "# Save the tokenizer (this creates tokenizer files)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"âœ“ Tokenizer saved (tokenizer.json, vocab.txt, etc. created)\")\n",
    "\n",
    "print(f\"\\nModel export completed successfully!\")\n",
    "print(f\"All files are now in: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450cb6c",
   "metadata": {},
   "source": [
    "## Step 6: Verify All Required Files Are Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad12e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created in the export directory:\n",
      "==================================================\n",
      "âœ“ README.md                 (0.00 MB)\n",
      "âœ“ config.json               (0.00 MB)\n",
      "âœ“ model.safetensors         (255.44 MB)\n",
      "âœ“ model_card.json           (0.00 MB)\n",
      "âœ“ special_tokens_map.json   (0.00 MB)\n",
      "âœ“ tokenizer.json            (0.68 MB)\n",
      "âœ“ tokenizer_config.json     (0.00 MB)\n",
      "âœ“ vocab.txt                 (0.22 MB)\n",
      "\n",
      "Required files for Hugging Face Hub:\n",
      "âœ“ model.safetensors - Present\n",
      "âœ“ config.json - Present\n",
      "âœ“ tokenizer.json - Present\n",
      "âœ“ tokenizer_config.json - Present\n",
      "âœ“ vocab.txt - Present\n",
      "âœ“ special_tokens_map.json - Present\n",
      "\n",
      "ðŸŽ‰ SUCCESS: All required files are present!\n",
      "Your model is ready to be uploaded to Hugging Face Hub.\n",
      "\n",
      "Note: model.safetensors is the newer, safer format preferred by Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "# List all files created\n",
    "print(\"Files created in the export directory:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for file_path in sorted(output_dir.iterdir()):\n",
    "    file_size = file_path.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    print(f\"âœ“ {file_path.name:<25} ({file_size:.2f} MB)\")\n",
    "\n",
    "print(\"\\nRequired files for Hugging Face Hub:\")\n",
    "required_files = [\n",
    "    'config.json', \n",
    "    'tokenizer.json',\n",
    "    'tokenizer_config.json',\n",
    "    'vocab.txt',\n",
    "    'special_tokens_map.json'\n",
    "]\n",
    "\n",
    "# Check for model files (either format is acceptable)\n",
    "model_files = ['pytorch_model.bin', 'model.safetensors']\n",
    "model_present = False\n",
    "\n",
    "for model_file in model_files:\n",
    "    file_path = output_dir / model_file\n",
    "    if file_path.exists():\n",
    "        print(f\"âœ“ {model_file} - Present\")\n",
    "        model_present = True\n",
    "        break\n",
    "\n",
    "if not model_present:\n",
    "    print(f\"âœ— Model file - Missing (expected one of: {', '.join(model_files)})\")\n",
    "\n",
    "all_present = model_present\n",
    "for required_file in required_files:\n",
    "    file_path = output_dir / required_file\n",
    "    if file_path.exists():\n",
    "        print(f\"âœ“ {required_file} - Present\")\n",
    "    else:\n",
    "        print(f\"âœ— {required_file} - Missing\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\nðŸŽ‰ SUCCESS: All required files are present!\")\n",
    "    print(\"Your model is ready to be uploaded to Hugging Face Hub.\")\n",
    "    print(\"\\nNote: model.safetensors is the newer, safer format preferred by Hugging Face.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  WARNING: Some required files are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4c4c2",
   "metadata": {},
   "source": [
    "## Step 7: Create Additional Metadata Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f50f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ README.md created\n",
      "âœ“ model_card.json created\n",
      "\n",
      "All files ready for Hugging Face Hub upload!\n"
     ]
    }
   ],
   "source": [
    "# Create a README.md file for the model\n",
    "readme_content = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: distilbert-base-uncased\n",
    "tags:\n",
    "- text-classification\n",
    "- log-analysis\n",
    "- openstack\n",
    "- distilbert\n",
    "- fine-tuned\n",
    "datasets:\n",
    "- custom\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: text-classification\n",
    "---\n",
    "\n",
    "# INFRNCE BERT Log Classification Model\n",
    "\n",
    "This is a fine-tuned DistilBERT model for classifying OpenStack Nova log entries into different operational categories.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: distilbert-base-uncased\n",
    "- **Task**: Multi-class text classification\n",
    "- **Number of Labels**: {len(label_encoder.classes_)}\n",
    "- **Domain**: OpenStack log analysis\n",
    "\n",
    "## Labels\n",
    "\n",
    "The model classifies logs into the following categories:\n",
    "\n",
    "{\", \".join([f\"- {label}\" for label in label_encoder.classes_])}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-username/infrnce-bert-log-classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"your-username/infrnce-bert-log-classifier\")\n",
    "\n",
    "# Example usage\n",
    "log_text = \"Your OpenStack log entry here\"\n",
    "inputs = tokenizer(log_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class_id = predictions.argmax().item()\n",
    "    \n",
    "print(f\"Predicted class: {{model.config.id2label[predicted_class_id]}}\")\n",
    "```\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The model was trained on a curated dataset of OpenStack Nova logs with both regex-based classifications and semantic clustering.\n",
    "\n",
    "## Performance\n",
    "\n",
    "The model was trained with controlled accuracy to achieve optimal performance on log classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Save README.md\n",
    "readme_path = output_dir / 'README.md'\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"âœ“ README.md created\")\n",
    "\n",
    "# Create a model card metadata file\n",
    "model_card = {\n",
    "    \"model_type\": \"distilbert\",\n",
    "    \"task\": \"text-classification\",\n",
    "    \"tags\": [\"log-analysis\", \"openstack\", \"text-classification\"],\n",
    "    \"base_model\": \"distilbert-base-uncased\",\n",
    "    \"num_labels\": len(label_encoder.classes_),\n",
    "    \"labels\": label_encoder.classes_.tolist(),\n",
    "    \"id2label\": id2label,\n",
    "    \"label2id\": label2id\n",
    "}\n",
    "\n",
    "# Save model card\n",
    "model_card_path = output_dir / 'model_card.json'\n",
    "with open(model_card_path, 'w') as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ model_card.json created\")\n",
    "print(f\"\\nAll files ready for Hugging Face Hub upload!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2e605",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Your model has been successfully exported! The `infrnce_bert_model_complete` directory now contains all the files needed to upload to Hugging Face Hub:\n",
    "\n",
    "1. **model.safetensors** - Your fine-tuned model weights (newer, safer format)\n",
    "2. **config.json** - Model architecture configuration\n",
    "3. **tokenizer.json** - Tokenizer configuration\n",
    "4. **vocab.txt** - Vocabulary file\n",
    "5. **tokenizer_config.json** - Tokenizer settings\n",
    "6. **special_tokens_map.json** - Special tokens mapping\n",
    "7. **README.md** - Model documentation\n",
    "8. **model_card.json** - Additional metadata\n",
    "\n",
    "You can now create a private repository on Hugging Face Hub and upload the entire contents of the `infrnce_bert_model_complete` directory.\n",
    "\n",
    "**Note**: The model was saved in the `model.safetensors` format, which is the newer, safer format preferred by Hugging Face over `pytorch_model.bin`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
