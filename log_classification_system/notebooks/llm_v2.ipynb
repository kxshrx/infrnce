{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d5d33-467e-4968-9883-66b66f23cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM v2: Overnight Bulletproof Processing (1000 logs)\n",
      "Unclassified logs available: 14972\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"LLM v2: Overnight Bulletproof Processing (1000 logs)\")\n",
    "\n",
    "checkpoint_dir = Path(\"llm_checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "df = pd.read_csv('../results/nova_logs_with_bert.csv')\n",
    "unclassified_logs = df[(df['regex_label'].isnull()) & (df['bert_label'].isnull())].copy()\n",
    "print(f\"Unclassified logs available: {len(unclassified_logs)}\")\n",
    "\n",
    "RESULTS_LIST = []\n",
    "PROCESSED_COUNT = 0\n",
    "CONSECUTIVE_FAILURES = 0\n",
    "MAX_CONSECUTIVE_FAILURES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7dd9d-69f6-4da3-8efd-72a40dc7cc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from: 0 logs already processed\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint_immediately(results_list, count):\n",
    "    if results_list:\n",
    "        results_data = []\n",
    "        for i, result in enumerate(results_list):\n",
    "            results_data.append({\n",
    "                'log_index': i,\n",
    "                'llm_category': result.category,\n",
    "                'llm_confidence': result.confidence,\n",
    "                'llm_reasoning': result.reasoning\n",
    "            })\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        results_df.to_csv(f'llm_v2_results_backup_{count}.csv', index=False)\n",
    "        print(f\"Checkpoint saved: {count} logs processed\")\n",
    "\n",
    "\n",
    "def load_existing_checkpoint(filename=\"llm_v2_checkpoint.pkl\"):\n",
    "    checkpoint_path = checkpoint_dir / filename\n",
    "    if checkpoint_path.exists():\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint_data = pickle.load(f)\n",
    "        print(f\"Checkpoint found: {checkpoint_data['processed_count']} logs previously processed\")\n",
    "        print(f\"Previous success rate: {checkpoint_data['success_rate']:.2%}\")\n",
    "        return checkpoint_data['results'], checkpoint_data['processed_count']\n",
    "    return [], 0\n",
    "\n",
    "RESULTS_LIST, PROCESSED_COUNT = load_existing_checkpoint()\n",
    "print(f\"Starting from: {PROCESSED_COUNT} logs already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f6a56-f2c8-49e0-8c93-11634fac2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategic sample created: 1000 logs to process\n"
     ]
    }
   ],
   "source": [
    "def create_strategic_1k_sample(unclassified_logs, target_size=1000, skip_processed=0):\n",
    "    strategic_sample = []\n",
    "    error_logs = unclassified_logs[\n",
    "        unclassified_logs['raw_log_text'].str.contains(\n",
    "            'ERROR|WARNING|CRITICAL|TIMEOUT|FAILED', case=False, na=False\n",
    "        )\n",
    "    ]\n",
    "    priority_1 = error_logs.sample(n=min(400, len(error_logs)), random_state=42)\n",
    "    strategic_sample.append(priority_1)\n",
    "    cluster_targets = {3: 200, 5: 150, 6: 150, 9: 50, 13: 50}\n",
    "    used_indices = priority_1.index\n",
    "    for cluster_id, target_count in cluster_targets.items():\n",
    "        cluster_logs = unclassified_logs[\n",
    "            (unclassified_logs['cluster_id'] == cluster_id) & \n",
    "            (~unclassified_logs.index.isin(used_indices))\n",
    "        ]\n",
    "        if len(cluster_logs) > 0:\n",
    "            sample_size = min(target_count, len(cluster_logs))\n",
    "            cluster_sample = cluster_logs.sample(n=sample_size, random_state=42)\n",
    "            strategic_sample.append(cluster_sample)\n",
    "            used_indices = used_indices.union(cluster_sample.index)\n",
    "    final_sample = pd.concat(strategic_sample, ignore_index=True)\n",
    "    if skip_processed > 0:\n",
    "        final_sample = final_sample.iloc[skip_processed:].reset_index(drop=True)\n",
    "        print(f\"Skipping {skip_processed} already processed logs\")\n",
    "    return final_sample\n",
    "\n",
    "strategic_1k_logs = create_strategic_1k_sample(unclassified_logs, target_size=1000, skip_processed=PROCESSED_COUNT)\n",
    "print(f\"Strategic sample created: {len(strategic_1k_logs)} logs to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dedc3-e704-41fd-98f4-eb4b907c0c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: /Users/kxshrx/dev/log-classification/.env\n",
      "Enhanced classification function with robust fallback created\n"
     ]
    }
   ],
   "source": [
    "class LogClassification(BaseModel):\n",
    "    category: str = Field(..., description=\"Classification category\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score\")\n",
    "    reasoning: str = Field(..., description=\"Brief explanation\")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "dotenv_path = os.path.abspath(os.path.join(os.getcwd(), '../.env'))\n",
    "print(f\"Loading .env from: {dotenv_path}\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "    model_name='llama-3.1-8b-instant',\n",
    "    temperature=0.3,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "optimized_template = \"\"\"Classify OpenStack log:\n",
    "\n",
    "CATEGORIES:\n",
    "SysOps, InstMgmt, NetOps, ResMgmt, SchedOps, BootErr, NetErr, FileErr, ConfigErr, ResErr, SvcErr\n",
    "\n",
    "EXAMPLES:\n",
    "- \"WARNING _wait_for_boot timeout\" → BootErr\n",
    "- \"INFO VIF plugged successfully\" → NetOps  \n",
    "- \"ERROR file not found\" → FileErr\n",
    "\n",
    "LOG: {log_message}\n",
    "\n",
    "Respond ONLY with a valid JSON object in the next line. Do NOT include any explanation, markdown, or formatting.\n",
    "EXAMPLE RESPONSE: {{\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"log_message\"], template=optimized_template)\n",
    "def classify_with_failure_detection(log_message, llm, prompt_template, max_retries=2):\n",
    "    import re\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            formatted_prompt = (\n",
    "                prompt_template.format(log_message=log_message[:400])\n",
    "                + \"\\nRespond ONLY with a valid JSON object in the next line. \"\n",
    "                + \"Do NOT include any explanation, markdown, or formatting.\\n\"\n",
    "                + 'EXAMPLE RESPONSE: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}'\n",
    "            )\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = llm.invoke(messages)\n",
    "            response_text = response.content.strip()\n",
    "            print(f\"LLM raw response: {response_text[:120]}\")\n",
    "\n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            result_data = None\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                try:\n",
    "                    result_data = json.loads(json_str)\n",
    "                except Exception:\n",
    "                    result_data = None\n",
    "            if not result_data:\n",
    "                try:\n",
    "                    result_data = json.loads(response_text)\n",
    "                except Exception:\n",
    "                    result_data = None\n",
    "            if not result_data:\n",
    "                cat_match = re.search(r\"\\*\\*(\\w+Err(?:ors)?)\\*\\*\", response_text)\n",
    "                category = cat_match.group(1) if cat_match else \"Processing_Error\"\n",
    "                result_data = {\n",
    "                    \"category\": category,\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"reasoning\": \"Parsed from non-JSON LLM output\"\n",
    "                }\n",
    "            category_mapping = {\n",
    "                'SysOps': 'System_Operations', 'InstMgmt': 'Instance_Management', \n",
    "                'NetOps': 'Network_Operations', 'ResMgmt': 'Resource_Management',\n",
    "                'SchedOps': 'Scheduler_Operations', 'BootErr': 'Boot_Timeout_Errors',\n",
    "                'NetErr': 'Network_Connection_Errors', 'FileErr': 'File_System_Errors',\n",
    "                'ConfigErr': 'Configuration_Errors', 'ResErr': 'Resource_Allocation_Errors',\n",
    "                'SvcErr': 'Service_Communication_Errors'\n",
    "            }\n",
    "            category = result_data.get('category', result_data.get('cat', 'Unknown'))\n",
    "            if category in category_mapping:\n",
    "                category = category_mapping[category]\n",
    "            result = LogClassification(\n",
    "                category=category,\n",
    "                confidence=result_data.get('confidence', result_data.get('conf', 0.0)),\n",
    "                reasoning=result_data.get('reasoning', 'Classified')\n",
    "            )\n",
    "            return result, False\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Retrying due to error: {e}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            print(f\"General exception: {e}\")\n",
    "            return LogClassification(\n",
    "                category=\"Processing_Error\",\n",
    "                confidence=0.0,\n",
    "                reasoning=f\"Error: {str(e)[:50]}\"\n",
    "            ), True\n",
    "\n",
    "print(\"Enhanced classification function with robust fallback created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb8a5d-871a-494d-82ca-4079c7e1b0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Start time: 00:31:48\n",
      "Starting processing from log 1/1000\n",
      "Early stopping after 5 consecutive failures\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"file not found in path\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest start failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event network-vif-plugged-30e24067-4dc4-48c5-8569-4d\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'file not found'\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.9, \"reasoning\": \"Instance spawn failure\"}\n",
      "Processed 10/1000 (1.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message indicates file not found\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"The log contains the string 'file not found' which is a common \n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.7, \"reasoning\": \"Error launching a defined domain\"}\n",
      "Processed 20/1000 (2.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"log contains the word 'boot' and the category is related to boo\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Unexpected event received during instance boot process\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Checkpoint saved: 25 logs processed\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service related error\"}\n",
      "Processed 30/1000 (3.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"error message indicates failure to build and run instance\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"instance state error\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by ERROR in nova.compute.manager\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Processed 40/1000 (4.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"vm_state building and task_state spawning indicate booting proc\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot process\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 1, \"reasoning\": \"file not found error in nova driver\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt.libvirtError indicates a boot issue\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s), not rescheduling.\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates boot failure\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 50/1000 (5.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"exception raised\"}\n",
      "Checkpoint saved: 50 logs processed\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"unexpected event during boot process\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "Processed 60/1000 (6.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 1.0, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 1.0, \"reasoning\": \"log contains ERROR and file not found keywords\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error indicates boot issue\"}\n",
      "Processed 70/1000 (7.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 1.0, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error indicates boot failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event for instance with vm_state building\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error\"}\n",
      "Checkpoint saved: 75 logs processed\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn due to VirtualInterfaceCreateException\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'file not found'\"}\n",
      "Processed 80/1000 (8.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"cleanup_instance_disks indicates a boot-related error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout error in boot process\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 1.0, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"timeout in boot process\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 90/1000 (9.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'Failed to build and run instance'\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event indicates boot issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"error in instance spawn\"}\n",
      "Processed 100/1000 (10.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error\"}\n",
      "Checkpoint saved: 100 logs processed\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout error during instance boot\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"Unexpected event for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"file not found error in nova compute manager\"}\n",
      "Processed 110/1000 (11.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found error indicates boot failure\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.7, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"cleanup_instance_disks indicates boot process failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"unexpected event for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "Processed 120/1000 (12.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error message indicates a service error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Checkpoint saved: 125 logs processed\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.6, \"reasoning\": \"cleanup_instance_disks indicates a boot related issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found error indicates a boot failure\"}\n",
      "Processed 130/1000 (13.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"error during stop() in sync_power_state\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout error during instance boot\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.8, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout error in boot process\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest start failure\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Error message indicates a failure to start a virtual machine\"}\n",
      "Processed 140/1000 (14.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error in nova.compute.manager\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event indicates boot issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"Error message indicates a service error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"disk not found error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 150/1000 (15.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Checkpoint saved: 150 logs processed\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"file not found in the specified path\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'file not found'\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"file not found error in libvirt.py\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "Processed 160/1000 (16.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Error launching a defined domain with XML indicates a boot erro\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"guest.launch(pause=pause) indicates boot failure\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"service error indication in log\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"log contains _wait_for_boot\"}\n",
      "Processed 170/1000 (17.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by ERROR level log\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.9, \"reasoning\": \"Instance spawn failure due to Virtual Interface creation error\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Checkpoint saved: 175 logs processed\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in nova service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 180/1000 (18.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError and instance boot failure\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"file not found in the specified path\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout in boot process\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Processed 190/1000 (19.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.7, \"reasoning\": \"createWithFlags method indicates a configuration issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest start failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s), not rescheduling.\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error indicates a boot issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "Processed 200/1000 (20.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"unexpected event received for instance in building state\"}\n",
      "Checkpoint saved: 200 logs processed\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Error launching a defined domain with XML\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 1.0, \"reasoning\": \"error message in log indicates file not found\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 210/1000 (21.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"service call failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found error\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"service error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "Processed 220/1000 (22.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"vm_state building and task_state spawning\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 1.0, \"reasoning\": \"file not found error in nova.compute.manager\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"domain not found error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"instance boot error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Checkpoint saved: 225 logs processed\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by ERROR and nova.compute.manager\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service related error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 230/1000 (23.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error message indicates a service error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error during instance boot\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.8, \"reasoning\": \"error in resource management\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Unexpected event for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found error indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 240/1000 (24.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event indicates boot issue\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Unexpected event indicates boot issue\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"file not found error in nova compute manager\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "Processed 250/1000 (25.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Checkpoint saved: 250 logs processed\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error during stop() in sync_power_state\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"timeout waiting for network-vif-plugged event\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.5, \"reasoning\": \"service error\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout error in boot process\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Boot error keywords: unexpected event, vm_state building, task_\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by 'ERROR' log level\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Retrying due to error: no healthy upstream\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 260/1000 (26.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout waiting for network-vif-plugged event\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"boot error keywords present in log\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.7, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by ERROR level log\"}\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.8, \"reasoning\": \"cleanup_instance_disks is not a valid function in nova.comput\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found error indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot issue\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 270/1000 (27.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"error in libvirt guest start\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event indicates boot issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Checkpoint saved: 275 logs processed\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.8, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "Processed 280/1000 (28.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout indicated in log\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"disk not found error\"}\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"nested exception\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"cleanup_instance_disks indicates an issue with instance boot pr\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Error launching a defined domain\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found\"}\n",
      "Processed 290/1000 (29.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error during instance boot\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance management error message\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.7, \"reasoning\": \"error in nova service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource allocation issue\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received for instance with vm_state building\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 300/1000 (30.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"boot error keywords present in log\"}\n",
      "Checkpoint saved: 300 logs processed\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.7, \"reasoning\": \"error message indicates service failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"instance state mismatch\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Domain not found\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.7, \"reasoning\": \"error in nova.compute.manager indicates a boot issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"unexpected event during instance boot\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'file not found'\"}\n",
      "Processed 310/1000 (31.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot error\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Unexpected event received during instance boot process\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.6, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt guest failed to start\"}\n",
      "Processed 320/1000 (32.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'file not found'\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "Checkpoint saved: 325 logs processed\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"timeout in boot process\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Unexpected event received for instance in building state\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Instance shutdown by itself\"}\n",
      "Processed 330/1000 (33.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"contains 'post_xml_callback' and 'ERROR'\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"log indicates periodic task execution\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"file not found error in nova driver\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in nova service\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"file not found in error message\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error indicates boot issue\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Processed 340/1000 (34.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"UnexpectedTaskStateError indicates a service error\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error in nova.compute.manager\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event received during instance boot\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 1.0, \"reasoning\": \"file not found in error message\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 1.0, \"reasoning\": \"Domain not found error indicates a boot error\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.9, \"reasoning\": \"error message contains 'file'\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by 'ERROR' log level and 'nova.compute.m\n",
      "Processed 350/1000 (35.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.8, \"reasoning\": \"resource error\"}\n",
      "Checkpoint saved: 350 logs processed\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"unexpected event for active instance\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate network(s)\"}\n",
      "Processed 360/1000 (36.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"unexpected event network-vif-unplugged\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"Error in nova service\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"Error in nova.compute.manager indicates a service error\"}\n",
      "Processed 370/1000 (37.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.8, \"reasoning\": \"return fn(self, *args, **kwargs) indicates a configuration er\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirtError indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"cleanup_instance_disks indicates a boot issue\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Checkpoint saved: 375 logs processed\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"power states mismatch\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "Processed 380/1000 (38.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"libvirt error indicates boot failure\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"vm_state building and task_state spawning\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indication in the log message\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn due to libvirt error\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"service error indicated by ERROR status\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"network allocation failed\"}\n",
      "Processed 390/1000 (39.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Error launching a defined domain with XML indicates a boot fail\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.9, \"reasoning\": \"error in service\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"FileErr\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.8, \"reasoning\": \"instance shutdown\"}\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"nova.compute.manager\"}\n",
      "Processed 400/1000 (40.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"SvcErr\", \"confidence\": 0.8, \"reasoning\": \"error type in log\"}\n",
      "Checkpoint saved: 400 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"Successfully plugged vif\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged or unplugged\"}\n",
      "Processed 410/1000 (41.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif operation\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged or unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 420/1000 (42.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Checkpoint saved: 425 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 430/1000 (43.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"success message with vif operation\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif operation\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"Successfully unplugged vif indicates network operations\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"Successfully unplugged vif indicates network operations\"}\n",
      "Processed 440/1000 (44.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"Successfully plugged vif\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 450/1000 (45.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Checkpoint saved: 450 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "Processed 460/1000 (46.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"Successfully plugged vif\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 470/1000 (47.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Checkpoint saved: 475 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"Successfully plugged vif\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 480/1000 (48.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 490/1000 (49.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif operation\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 500/1000 (50.0%) - Success rate: 100.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Checkpoint saved: 500 logs processed\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500107, Requested 268. Please try again in 1m4.9678s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500095, Requested 268. Please try again in 1m2.8518s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500083, Requested 268. Please try again in 1m0.7728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500115, Requested 273. Please try again in 1m7.0484s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500103, Requested 273. Please try again in 1m4.975399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500090, Requested 273. Please try again in 1m2.8924s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500076, Requested 273. Please try again in 1m0.3174s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500108, Requested 273. Please try again in 1m5.8688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500096, Requested 273. Please try again in 1m3.7738s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500083, Requested 273. Please try again in 1m1.6708s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500102, Requested 273. Please try again in 1m4.805s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500089, Requested 273. Please try again in 1m2.688999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500077, Requested 273. Please try again in 1m0.611s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500109, Requested 269. Please try again in 1m5.3284s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500096, Requested 269. Please try again in 1m3.2134s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500084, Requested 269. Please try again in 1m1.1354s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 510/1000 (51.0%) - Success rate: 99.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500113, Requested 273. Please try again in 1m6.8546s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 273. Please try again in 1m4.7826s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500089, Requested 273. Please try again in 1m2.6716s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500074, Requested 273. Please try again in 1m0.090599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500114, Requested 273. Please try again in 1m6.8786s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 273. Please try again in 1m4.789599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500089, Requested 273. Please try again in 1m2.7116s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500074, Requested 273. Please try again in 1m0.096599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500117, Requested 273. Please try again in 1m7.399s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500104, Requested 273. Please try again in 1m5.285s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500092, Requested 273. Please try again in 1m3.173s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500077, Requested 273. Please try again in 1m0.593s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500122, Requested 273. Please try again in 1m8.263999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500110, Requested 273. Please try again in 1m6.182999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 273. Please try again in 1m4.104999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500083, Requested 273. Please try again in 1m1.525s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500110, Requested 269. Please try again in 1m5.4962s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 269. Please try again in 1m3.415199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500085, Requested 269. Please try again in 1m1.301199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 520/1000 (52.0%) - Success rate: 98.1%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500106, Requested 269. Please try again in 1m4.810999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500094, Requested 269. Please try again in 1m2.735s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500081, Requested 269. Please try again in 1m0.652s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"log indicates successful network operation\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500116, Requested 273. Please try again in 1m7.2242s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500103, Requested 273. Please try again in 1m5.1422s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500091, Requested 273. Please try again in 1m3.0282s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500076, Requested 273. Please try again in 1m0.445199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500107, Requested 273. Please try again in 1m5.669s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500095, Requested 273. Please try again in 1m3.593s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500082, Requested 273. Please try again in 1m1.51s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Checkpoint saved: 525 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500108, Requested 273. Please try again in 1m5.9836s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500096, Requested 273. Please try again in 1m3.898599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500084, Requested 273. Please try again in 1m1.8146s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500114, Requested 268. Please try again in 1m6.157399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500102, Requested 268. Please try again in 1m4.0804s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500090, Requested 268. Please try again in 1m1.9974s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 530/1000 (53.0%) - Success rate: 97.2%\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500075, Requested 273. Please try again in 1m0.2844s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500112, Requested 268. Please try again in 1m5.8278s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500100, Requested 268. Please try again in 1m3.7508s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500088, Requested 268. Please try again in 1m1.6718s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500117, Requested 273. Please try again in 1m7.404s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500105, Requested 273. Please try again in 1m5.331s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500093, Requested 273. Please try again in 1m3.259999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500113, Requested 273. Please try again in 1m6.719799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 273. Please try again in 1m4.6498s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500088, Requested 273. Please try again in 1m2.5428s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.8, \"reasoning\": \"log is related to configuration\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500114, Requested 269. Please try again in 1m6.334199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500102, Requested 269. Please try again in 1m4.256199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500090, Requested 269. Please try again in 1m2.1762s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 540/1000 (54.0%) - Success rate: 96.5%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500109, Requested 268. Please try again in 1m5.2894s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 268. Please try again in 1m3.2144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500085, Requested 268. Please try again in 1m1.1344s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500106, Requested 273. Please try again in 1m5.5102s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500094, Requested 273. Please try again in 1m3.4552s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500082, Requested 273. Please try again in 1m1.3922s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500109, Requested 273. Please try again in 1m6.145399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 273. Please try again in 1m4.0744s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500085, Requested 273. Please try again in 1m1.9594s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500115, Requested 273. Please try again in 1m7.217199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500103, Requested 273. Please try again in 1m5.1092s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500091, Requested 273. Please try again in 1m2.9962s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500076, Requested 273. Please try again in 1m0.418199999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500109, Requested 273. Please try again in 1m6.0106s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 273. Please try again in 1m3.9376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500084, Requested 273. Please try again in 1m1.8606s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 550/1000 (55.0%) - Success rate: 95.6%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Checkpoint saved: 550 logs processed\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500112, Requested 273. Please try again in 1m6.6788s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500100, Requested 273. Please try again in 1m4.5968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500088, Requested 273. Please try again in 1m2.514799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500116, Requested 269. Please try again in 1m6.542999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500103, Requested 269. Please try again in 1m4.441s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500091, Requested 269. Please try again in 1m2.368999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500076, Requested 273. Please try again in 1m0.4562s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500117, Requested 269. Please try again in 1m6.703799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500105, Requested 269. Please try again in 1m4.636799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500093, Requested 269. Please try again in 1m2.559799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500078, Requested 273. Please try again in 1m0.665s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500109, Requested 269. Please try again in 1m5.4662s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 269. Please try again in 1m3.4032s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500085, Requested 269. Please try again in 1m1.3262s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500114, Requested 273. Please try again in 1m6.8836s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500102, Requested 273. Please try again in 1m4.8146s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500090, Requested 273. Please try again in 1m2.737599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 560/1000 (56.0%) - Success rate: 94.8%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500110, Requested 273. Please try again in 1m6.1974s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 273. Please try again in 1m4.0634s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500085, Requested 273. Please try again in 1m1.9764s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500105, Requested 273. Please try again in 1m5.325399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500093, Requested 273. Please try again in 1m3.2454s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500080, Requested 273. Please try again in 1m1.160399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500113, Requested 269. Please try again in 1m6.0156s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 269. Please try again in 1m3.9386s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500088, Requested 269. Please try again in 1m1.8616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500112, Requested 273. Please try again in 1m6.54s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500099, Requested 273. Please try again in 1m4.422s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500087, Requested 273. Please try again in 1m2.324s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500112, Requested 269. Please try again in 1m5.847799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500100, Requested 269. Please try again in 1m3.7738s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500088, Requested 269. Please try again in 1m1.6948s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Processed 570/1000 (57.0%) - Success rate: 94.0%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500115, Requested 273. Please try again in 1m7.0584s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500103, Requested 273. Please try again in 1m4.9734s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500090, Requested 273. Please try again in 1m2.8934s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500107, Requested 273. Please try again in 1m5.673s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500095, Requested 273. Please try again in 1m3.591999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500082, Requested 273. Please try again in 1m1.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500113, Requested 273. Please try again in 1m6.7188s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 273. Please try again in 1m4.636799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500089, Requested 273. Please try again in 1m2.557799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Checkpoint saved: 575 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500111, Requested 268. Please try again in 1m5.647s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500099, Requested 268. Please try again in 1m3.568s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500087, Requested 268. Please try again in 1m1.458999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500094, Requested 273. Please try again in 1m3.5804s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500082, Requested 273. Please try again in 1m1.4974s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 580/1000 (58.0%) - Success rate: 93.4%\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500108, Requested 268. Please try again in 1m4.981799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500096, Requested 268. Please try again in 1m2.9038s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500083, Requested 268. Please try again in 1m0.8228s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500102, Requested 269. Please try again in 1m4.2796s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500090, Requested 269. Please try again in 1m2.1996s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500078, Requested 269. Please try again in 1m0.0856s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500122, Requested 268. Please try again in 1m7.412s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500110, Requested 268. Please try again in 1m5.331999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500098, Requested 268. Please try again in 1m3.248999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500083, Requested 273. Please try again in 1m1.532s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500109, Requested 273. Please try again in 1m6.0276s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500097, Requested 273. Please try again in 1m3.9466s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500084, Requested 273. Please try again in 1m1.837599999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Retrying due to error: no healthy upstream\n",
      "Retrying due to error: Connection error.\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\n",
      "  \"category\": \"NetOps\",\n",
      "  \"confidence\": 0.9,\n",
      "  \"reasoning\": \"vif plugged successfully\"\n",
      "}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"action related to network interface\"}\n",
      "Processed 590/1000 (59.0%) - Success rate: 92.9%\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500117, Requested 273. Please try again in 1m7.399s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500104, Requested 273. Please try again in 1m5.287s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500092, Requested 273. Please try again in 1m3.177s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500112, Requested 269. Please try again in 1m5.8498s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500100, Requested 269. Please try again in 1m3.7808s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500088, Requested 269. Please try again in 1m1.7108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500117, Requested 273. Please try again in 1m7.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500105, Requested 273. Please try again in 1m5.325s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500092, Requested 273. Please try again in 1m3.243s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500107, Requested 273. Please try again in 1m5.806799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500095, Requested 273. Please try again in 1m3.7208s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500083, Requested 273. Please try again in 1m1.635799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500107, Requested 269. Please try again in 1m4.9768s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500094, Requested 269. Please try again in 1m2.8338s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500082, Requested 269. Please try again in 1m0.7078s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 600/1000 (60.0%) - Success rate: 92.2%\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500105, Requested 269. Please try again in 1m4.794s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500093, Requested 269. Please try again in 1m2.71s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500081, Requested 269. Please try again in 1m0.585s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Checkpoint saved: 600 logs processed\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claims on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"nova.compute.claims indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node\"}\n",
      "Processed 610/1000 (61.0%) - Success rate: 92.1%\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"nova.compute.claims indicates a scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "Processed 620/1000 (62.0%) - Success rate: 92.2%\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1, \"reasoning\": \"Claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful indicates instance management\"}\n",
      "Checkpoint saved: 625 logs processed\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance management related\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates scheduling\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance management\"}\n",
      "Processed 630/1000 (63.0%) - Success rate: 92.4%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node indicates instance managemen\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates resource management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"nova.compute.claims indicates instance management\"}\n",
      "Processed 640/1000 (64.0%) - Success rate: 92.5%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"nova.compute.claims indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "Processed 650/1000 (65.0%) - Success rate: 92.6%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "Checkpoint saved: 650 logs processed\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance management related\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "Processed 660/1000 (66.0%) - Success rate: 92.7%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claims on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on a node indicates instance managem\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "Processed 670/1000 (67.0%) - Success rate: 92.8%\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.8, \"reasoning\": \"successful claim on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance management log\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful on node\"}\n",
      "Checkpoint saved: 675 logs processed\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "Processed 680/1000 (68.0%) - Success rate: 92.9%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"successful claim on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance management category matches 'nova.compute.claims'\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "Processed 690/1000 (69.0%) - Success rate: 93.0%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance management related\"}\n",
      "Processed 700/1000 (70.0%) - Success rate: 93.1%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "Checkpoint saved: 700 logs processed\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"nova.compute.claims indicates instance management\"}\n",
      "Processed 710/1000 (71.0%) - Success rate: 93.2%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance management related\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"nova.compute.claims indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "Processed 720/1000 (72.0%) - Success rate: 93.3%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "Checkpoint saved: 725 logs processed\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node\"}\n",
      "Processed 730/1000 (73.0%) - Success rate: 93.4%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node indicates instance managemen\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful on node\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates scheduling operation\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance management related\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"Nova compute claims instance\"}\n",
      "Processed 740/1000 (74.0%) - Success rate: 93.5%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance management log\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance claim successful\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates scheduling operation\"}\n",
      "Processed 750/1000 (75.0%) - Success rate: 93.6%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"claim successful on node indicates instance management\"}\n",
      "Checkpoint saved: 750 logs processed\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 760/1000 (76.0%) - Success rate: 93.7%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ConfigErr\", \"confidence\": 0.6, \"reasoning\": \"script error\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "Processed 770/1000 (77.0%) - Success rate: 93.8%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deleted for instance\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "Checkpoint saved: 775 logs processed\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "Processed 780/1000 (78.0%) - Success rate: 93.8%\n",
      "LLM raw response: {\"category\": \"InstMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"Resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "Retrying due to error: Error code: 503 - {'error': {'message': 'Service unavailable. We are working on a fix!', 'type': 'internal_server_error'}}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"deleted allocation for instance\"}\n",
      "Processed 790/1000 (79.0%) - Success rate: 93.9%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"log indicates resource management operation\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 800/1000 (80.0%) - Success rate: 94.0%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Checkpoint saved: 800 logs processed\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"Instance deletion\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 810/1000 (81.0%) - Success rate: 94.1%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"log contains 'Deleted allocation for instance'\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 820/1000 (82.0%) - Success rate: 94.1%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Checkpoint saved: 825 logs processed\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 830/1000 (83.0%) - Success rate: 94.2%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"Resource management operation\"}\n",
      "Processed 840/1000 (84.0%) - Success rate: 94.3%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation and instance management\"}\n",
      "Processed 850/1000 (85.0%) - Success rate: 94.3%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation for instance\"}\n",
      "Checkpoint saved: 850 logs processed\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "Processed 860/1000 (86.0%) - Success rate: 94.4%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"log indicates deletion of allocation for instance\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"log contains 'Deleted allocation'\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"management of resources\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 870/1000 (87.0%) - Success rate: 94.5%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "Checkpoint saved: 875 logs processed\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.8, \"reasoning\": \"brief\"}\n",
      "Processed 880/1000 (88.0%) - Success rate: 94.5%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"resource management related\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"management operation\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"management of resources\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"resource management\"}\n",
      "Processed 890/1000 (89.0%) - Success rate: 94.6%\n",
      "Retrying due to error: no healthy upstream\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"instance deletion\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 0.9, \"reasoning\": \"allocation deletion\"}\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "Processed 900/1000 (90.0%) - Success rate: 94.7%\n",
      "LLM raw response: {\"category\": \"ResMgmt\", \"confidence\": 1.0, \"reasoning\": \"allocation deletion\"}\n",
      "Checkpoint saved: 900 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged or unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "Processed 910/1000 (91.0%) - Success rate: 94.7%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "Processed 920/1000 (92.0%) - Success rate: 94.8%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "Checkpoint saved: 925 logs processed\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "Processed 930/1000 (93.0%) - Success rate: 94.8%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged/unplugged\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"Successfully unplugged vif indicates network operations\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "Processed 940/1000 (94.0%) - Success rate: 94.9%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif plugged successfully\"}\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 1.0, \"reasoning\": \"vif unplugged successfully\"}\n",
      "Processed 950/1000 (95.0%) - Success rate: 94.9%\n",
      "LLM raw response: {\"category\": \"NetOps\", \"confidence\": 0.9, \"reasoning\": \"vif unplugged successfully\"}\n",
      "Checkpoint saved: 950 logs processed\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "Processed 960/1000 (96.0%) - Success rate: 95.0%\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"SchedOps\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "Processed 970/1000 (97.0%) - Success rate: 95.0%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate network(s) indicates a boot failure\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500106, Requested 271. Please try again in 1m5.3094s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500094, Requested 271. Please try again in 1m3.231399999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500082, Requested 271. Please try again in 1m1.1434s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "Checkpoint saved: 975 logs processed\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "Processed 980/1000 (98.0%) - Success rate: 95.0%\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 271. Please try again in 1m4.3964s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500089, Requested 271. Please try again in 1m2.3154s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500077, Requested 271. Please try again in 1m0.2324s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.7, \"reasoning\": \"Virtual Interface creation failed indicates boot process failur\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "Processed 990/1000 (99.0%) - Success rate: 94.9%\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"ResErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 1.0, \"reasoning\": \"Failed to allocate the network(s)\"}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500113, Requested 271. Please try again in 1m6.526s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Retrying due to error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500101, Requested 271. Please try again in 1m4.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ General exception: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jy1tjf2hextbeqk1b4ejmhgs` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500089, Requested 271. Please try again in 1m2.348s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " Failure 1/5: Processing_Error\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Virtual Interface creation failed\"}\n",
      "LLM raw response: {\"category\": \"BootErr\", \"confidence\": 0.9, \"reasoning\": \"Instance failed to spawn\"}\n",
      "Processed 1000/1000 (100.0%) - Success rate: 94.9%\n",
      "LLM raw response: {\"category\": \"NetErr\", \"confidence\": 0.9, \"reasoning\": \"Failed to allocate network(s)\"}\n",
      "Checkpoint saved: 1000 logs processed\n",
      "Checkpoint saved: 1000 logs processed\n",
      "\n",
      "Processing completed: 1000 total classifications\n",
      "⏰ End time: 07:52:29\n",
      "⏱️ Duration: 7:20:40.895769\n"
     ]
    }
   ],
   "source": [
    "def process_with_early_stopping_and_checkpoints(logs_list, llm, prompt_template, start_from=0):\n",
    "    global RESULTS_LIST, PROCESSED_COUNT, CONSECUTIVE_FAILURES\n",
    "    total_logs = len(logs_list)\n",
    "    print(f\"Starting processing from log {start_from + 1}/{total_logs}\")\n",
    "    print(f\"Early stopping after {MAX_CONSECUTIVE_FAILURES} consecutive failures\")\n",
    "    for idx in range(start_from, total_logs):\n",
    "        log_message = logs_list[idx]\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            success_rate = len([r for r in RESULTS_LIST if r.category not in ['Processing_Error', 'Rate_Limit_Error']]) / max(len(RESULTS_LIST), 1)\n",
    "            print(f\"Processed {idx + 1}/{total_logs} ({(idx+1)/total_logs*100:.1f}%) - Success rate: {success_rate:.1%}\")\n",
    "        result, is_failure = classify_with_failure_detection(log_message, llm, prompt_template)\n",
    "        RESULTS_LIST.append(result)\n",
    "        PROCESSED_COUNT += 1\n",
    "        if is_failure:\n",
    "            CONSECUTIVE_FAILURES += 1\n",
    "            print(f\" Failure {CONSECUTIVE_FAILURES}/{MAX_CONSECUTIVE_FAILURES}: {result.category}\")\n",
    "            if CONSECUTIVE_FAILURES >= MAX_CONSECUTIVE_FAILURES:\n",
    "                print(f\"\\nEARLY STOPPING: {CONSECUTIVE_FAILURES} consecutive failures detected\")\n",
    "                print(f\" Processed {len(RESULTS_LIST)} logs before stopping\")\n",
    "                break\n",
    "        else:\n",
    "            CONSECUTIVE_FAILURES = 0\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            save_checkpoint_immediately(RESULTS_LIST, PROCESSED_COUNT)\n",
    "        time.sleep(2.5)\n",
    "    save_checkpoint_immediately(RESULTS_LIST, PROCESSED_COUNT)\n",
    "    print(f\"\\nProcessing completed: {len(RESULTS_LIST)} total classifications\")\n",
    "    return RESULTS_LIST\n",
    "\n",
    "strategic_logs_list = strategic_1k_logs['raw_log_text'].tolist()\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Start time: {start_time.strftime('%H:%M:%S')}\")\n",
    "final_results = process_with_early_stopping_and_checkpoints(\n",
    "    strategic_logs_list, \n",
    "    llm, \n",
    "    prompt,\n",
    "    start_from=0\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "print(f\"End time: {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Duration: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06723f-92e4-44ba-aa54-8aea70be4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL RESULTS ANALYSIS\n",
      "==================================================\n",
      "Total processed: 1000 logs\n",
      "\n",
      "🏷️ Category Distribution:\n",
      "  Network_Operations: 201 (20.1%)\n",
      "  Boot_Timeout_Errors: 187 (18.7%)\n",
      "  Resource_Management: 141 (14.1%)\n",
      "  Instance_Management: 136 (13.6%)\n",
      "  File_System_Errors: 96 (9.6%)\n",
      "  Service_Communication_Errors: 91 (9.1%)\n",
      "  Network_Connection_Errors: 55 (5.5%)\n",
      "  Processing_Error: 51 (5.1%)\n",
      "  Scheduler_Operations: 26 (2.6%)\n",
      "  Resource_Allocation_Errors: 9 (0.9%)\n",
      "  Configuration_Errors: 7 (0.7%)\n",
      "\n",
      "Confidence Analysis:\n",
      "  Average confidence: 0.856\n",
      "  High confidence (≥0.65): 945/1000 (94.5%)\n",
      "\n",
      "Success Rate: 94.9% (949/1000)\n",
      "Errors encountered: 496 (49.6%)\n",
      "\n",
      "FILES SAVED:\n",
      "llm_v2_final_results_1000.csv (1000 classifications)\n",
      "strategic_1k_with_llm_results.csv (integration ready)\n",
      "Checkpoint files in llm_checkpoints/ directory\n",
      "\n",
      "LLM v2 PROCESSING COMPLETE\n",
      "Successfully processed 1000 logs with bulletproof error handling\n"
     ]
    }
   ],
   "source": [
    "def safe_analyze_results(results_list):\n",
    "    if not results_list:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    print(f\"\\nFINAL RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    categories = [r.category for r in results_list]\n",
    "    confidences = [r.confidence for r in results_list if isinstance(r.confidence, (int, float))]\n",
    "    category_dist = Counter(categories)\n",
    "    print(f\"Total processed: {len(results_list)} logs\")\n",
    "    print(f\"\\nCategory Distribution:\")\n",
    "    for category, count in category_dist.most_common():\n",
    "        percentage = count / len(results_list) * 100\n",
    "        print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "    if confidences:\n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        high_conf_count = len([c for c in confidences if c >= 0.65])\n",
    "        print(f\"\\nConfidence Analysis:\")\n",
    "        print(f\"  Average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"  High confidence (≥0.65): {high_conf_count}/{len(confidences)} ({high_conf_count/len(confidences)*100:.1f}%)\")\n",
    "    successful_classifications = len([r for r in results_list if r.category not in ['Processing_Error', 'Rate_Limit_Error']])\n",
    "    success_rate = successful_classifications / len(results_list) * 100\n",
    "    print(f\"\\nSuccess Rate: {success_rate:.1f}% ({successful_classifications}/{len(results_list)})\")\n",
    "    errors = len([r for r in results_list if 'Error' in r.category])\n",
    "    if errors > 0:\n",
    "        print(f\"Errors encountered: {errors} ({errors/len(results_list)*100:.1f}%)\")\n",
    "\n",
    "safe_analyze_results(RESULTS_LIST)\n",
    "\n",
    "if RESULTS_LIST:\n",
    "    final_results_data = []\n",
    "    for i, result in enumerate(RESULTS_LIST):\n",
    "        final_results_data.append({\n",
    "            'log_index': i,\n",
    "            'llm_category': result.category,\n",
    "            'llm_confidence': result.confidence,\n",
    "            'llm_reasoning': result.reasoning\n",
    "        })\n",
    "    final_df = pd.DataFrame(final_results_data)\n",
    "    final_df.to_csv('../results/llm_v2_final_results_1000.csv', index=False)\n",
    "    processed_logs = strategic_1k_logs.head(len(RESULTS_LIST)).copy()\n",
    "    processed_logs['llm_category'] = [r.category for r in RESULTS_LIST]\n",
    "    processed_logs['llm_confidence'] = [r.confidence for r in RESULTS_LIST]\n",
    "    processed_logs['llm_reasoning'] = [r.reasoning for r in RESULTS_LIST]\n",
    "    processed_logs.to_csv('../results/strategic_1k_with_llm_results.csv', index=False)\n",
    "    print(f\"\\nFILES SAVED:\")\n",
    "    print(f\"llm_v2_final_results_1000.csv ({len(final_df)} classifications)\")\n",
    "    print(f\"strategic_1k_with_llm_results.csv (integration ready)\")\n",
    "    print(f\"Checkpoint files in llm_checkpoints/ directory\")\n",
    "\n",
    "print(f\"\\nLLM v2 PROCESSING COMPLETE\")\n",
    "print(f\"Successfully processed {len(RESULTS_LIST)} logs with bulletproof error handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51710878-caee-427c-84d0-a04406d1e404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
